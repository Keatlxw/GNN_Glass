{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bd569-5560-4f7a-9155-4af7c746b5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################Save_data_success#############################################################\n",
      "########################train_batch_tensor <torch.utils.data.dataset.Subset object at 0x7f4d37c540d0>\n",
      "epoch 0\n",
      "losses 73.29076385498047\n",
      "test_losses 4.808205604553223\n",
      "save_epoch: 0\n",
      "epoch 1\n",
      "losses 3.722734212875366\n",
      "test_losses 0.31362947821617126\n",
      "epoch 2\n",
      "losses 0.5975677371025085\n",
      "test_losses 0.1292521357536316\n",
      "epoch 3\n",
      "losses 0.285690575838089\n",
      "test_losses 0.07177551835775375\n",
      "[1353, 2413, 952, 1259, 1128, 142, 1497, 3060, 1975, 3097, 3765, 2096, 2524, 1470, 1022, 1153, 3703, 1800, 1298, 1309, 1745, 764, 2454, 2716, 3587, 2927, 1686, 2585, 345, 2758, 3470, 3377, 18, 138, 927, 3943, 3526, 1491, 1276, 2479, 3173, 2494, 1993, 323, 2476, 1413, 2680, 2847, 140, 67, 3222, 2371, 2117, 476, 3977, 240, 3213, 3960, 2459, 3428, 118, 990, 1575, 2464, 495, 2939, 2639, 2149, 3853, 1566, 3695, 1156, 236, 2289, 2949, 3473, 3516, 3058, 2912, 3498, 1704, 2367, 3739, 722, 3826, 2411, 2088, 2767, 1209, 208, 2336, 874, 2427, 3216, 2756, 3896, 3664, 630, 775, 1722]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "import argparse\n",
    "from utils import Normalizer\n",
    "from copy import deepcopy\n",
    "from utils import classification_eval\n",
    "'''from data_process import load_data_coordinate\n",
    "\n",
    "from data_process import load_feature_matrix\n",
    "\n",
    "from data_process import padding_zeros_matrix\n",
    "\n",
    "from data_process import edge_number_matrix\n",
    "\n",
    "#from data_process import GFA_label'''\n",
    "\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from layer import GNNLayer\n",
    "\n",
    "from remodel import GNNModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "class GaussianDistance(object):\n",
    "    \"\"\"\n",
    "    Expands the distance by Gaussian basis.\n",
    "    Inspired by Cgcnn, \"https://github.com/txie-93/cgcnn\"\n",
    "    \"\"\"\n",
    "    def __init__(self, dmin, dmax, step, var=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dmin (float): Minimum interatomic distance\n",
    "            dmax (float): Maximum interatomic distance\n",
    "            step (float): Step size for the Gaussian filter\n",
    "        \"\"\"\n",
    "        assert dmin < dmax\n",
    "        assert dmax - dmin > step\n",
    "        self.filter = np.arange(dmin, dmax + step, step)\n",
    "        self.var = var if var else step\n",
    "\n",
    "    def expand(self, distances):\n",
    "        \"\"\"\n",
    "        Apply Gaussian distance filter to a distance array\n",
    "        Args:\n",
    "            distances (Array, (N)): A distance array\n",
    "\n",
    "        Returns:\n",
    "            expanded_distance (Array, (N, len(self.filter)): Expanded distance\n",
    "                array using the Gaussian filter.\n",
    "        \"\"\"\n",
    "        return np.exp(-(np.expand_dims(distances, axis=-1) - self.filter)**2 /\n",
    "                      self.var**2)\n",
    "\n",
    "\n",
    "\n",
    "#edge_feature_matrix_list,neighbor_node_number_matrix_list,neighbor_node_index_matrix_list,node_feature_matrix_list,contre_node_index_matrix_list = load_feature_matrix(num_samples=100,\n",
    "                                                                                                                      #atom_number=4000,coordinate_number=3,threshold=4.0)\n",
    "\n",
    "\n",
    "#np.save(\"cu5zr5_edge_feature_matrix_list.npy\", edge_feature_matrix_list)\n",
    "\n",
    "#np.save(\"cu5zr5_node_feature_matrix_list.npy\", node_feature_matrix_list)\n",
    "\n",
    "#np.save(\"cu5zr5_neighbor_node_number_matrix_list.npy\",neighbor_node_number_matrix_list)\n",
    "\n",
    "#np.save(\"cu5zr5_neighbor_node_index_matrix_list.npy\",neighbor_node_index_matrix_list)\n",
    "\n",
    "#np.save(\"cu5zr5_contre_node_index_matrix_list.npy\",contre_node_index_matrix_list)\n",
    "\n",
    "\n",
    "print('###################################################Save_data_success#############################################################')\n",
    "\n",
    "#edge_feature_matrix_list=np.load(\"edge_feature_matrix_list.npy\")\n",
    "\n",
    "#node_feature_matrix_list=np.load(\"node_feature_matrix_list.npy\")\n",
    "\n",
    "#neighbor_node_number_matrix_list=np.load(\"neighbor_node_number_matrix_list.npy\")\n",
    "\n",
    "#neighbor_node_index_matrix_list=np.load(\"neighbor_node_index_matrix_list.npy\")\n",
    "\n",
    "#contre_node_index_matrix_list=np.load(\"contre_node_index_matrix_list.npy\",allow_pickle=True)\n",
    "\n",
    "cu5zr5_edge_feature_matrix_list = np.load(\"cu5zr5_edge_feature_matrix_list.npy\")\n",
    "\n",
    "cu5zr5_node_feature_matrix_list = np.load(\"cu5zr5_node_feature_matrix_list.npy\")\n",
    "\n",
    "cu5zr5_neighbor_node_index_matrix_list = np.load(\"cu5zr5_neighbor_node_index_matrix_list.npy\")\n",
    "\n",
    "cu5zr5_neighbor_node_number_matrix_list = np.load(\"cu5zr5_neighbor_node_number_matrix_list.npy\")\n",
    "\n",
    "\n",
    "ni5al5_edge_feature_matrix_list = np.load(\"ni5al5_edge_feature_matrix_list.npy\")\n",
    "\n",
    "ni5al5_node_feature_matrix_list = np.load(\"ni5al5_node_feature_matrix_list.npy\")\n",
    "\n",
    "ni5al5_neighbor_node_index_matrix_list = np.load(\"ni5al5_neighbor_node_index_matrix_list.npy\")\n",
    "\n",
    "ni5al5_neighbor_node_number_matrix_list = np.load(\"ni5al5_neighbor_node_number_matrix_list.npy\")\n",
    "\n",
    "\n",
    "edge_feature_matrix_list = []\n",
    "\n",
    "node_feature_matrix_list = []\n",
    "\n",
    "neighbor_node_number_matrix_list = []\n",
    "\n",
    "neighbor_node_index_matrix_list = []\n",
    "\n",
    "\n",
    "num_graphs = 200\n",
    "nodes_per_graph = 4000\n",
    "\n",
    "# 创建一个包含所有节点的列表\n",
    "all_nodes = []\n",
    "for graph_id in range(num_graphs):\n",
    "    graph_nodes = [graph_id] * nodes_per_graph  # 使用图的ID来表示节点所属的图\n",
    "    all_nodes.extend(graph_nodes)\n",
    "\n",
    "# 将列表转换为NumPy数组\n",
    "# 定义图和节点的数量\n",
    "num_graphs = 200\n",
    "nodes_per_graph = 4000\n",
    "\n",
    "# 创建一个包含所有节点的列表\n",
    "all_nodes = []\n",
    "for graph_id in range(num_graphs):\n",
    "    graph_nodes = [graph_id] * nodes_per_graph  # 使用图的ID来表示节点所属的图\n",
    "    all_nodes.extend(graph_nodes)\n",
    "\n",
    "# 将列表转换为NumPy数组\n",
    "batch = np.array(all_nodes)\n",
    "batch=batch.reshape(200,4000,1)\n",
    "\n",
    "# 打印批次的形状\n",
    "#print(\"Batch shape:\", batch.shape)\n",
    "\n",
    "#print(batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(cu5zr5_edge_feature_matrix_list)):\n",
    "\n",
    "    edge_feature_matrix_list.append(cu5zr5_edge_feature_matrix_list[i])\n",
    "\n",
    "for i in range(len(ni5al5_edge_feature_matrix_list)):\n",
    "\n",
    "    edge_feature_matrix_list.append(ni5al5_edge_feature_matrix_list[i])\n",
    "\n",
    "#print('cu5zr5_edge_feature_matrix_list[0][0].size',cu5zr5_edge_feature_matrix_list[0][0].size)\n",
    "\n",
    "#print('ni5al5_edge_feature_matrix_list[0][0].size',ni5al5_edge_feature_matrix_list[0][0].size)\n",
    "\n",
    "\n",
    "for i in range(len(cu5zr5_node_feature_matrix_list)):\n",
    "\n",
    "    node_feature_matrix_list.append(cu5zr5_node_feature_matrix_list[i])\n",
    "\n",
    "for i in range(len(ni5al5_node_feature_matrix_list)):\n",
    "\n",
    "    node_feature_matrix_list.append(ni5al5_node_feature_matrix_list[i])\n",
    "\n",
    "\n",
    "for i in range(len(cu5zr5_neighbor_node_index_matrix_list)):\n",
    "\n",
    "    neighbor_node_index_matrix_list.append(cu5zr5_neighbor_node_index_matrix_list[i])\n",
    "\n",
    "for i in range(len(ni5al5_neighbor_node_index_matrix_list)):\n",
    "\n",
    "    neighbor_node_index_matrix_list.append(ni5al5_neighbor_node_index_matrix_list[i])\n",
    "\n",
    "\n",
    "for i in range(len(cu5zr5_neighbor_node_number_matrix_list)):\n",
    "\n",
    "    neighbor_node_number_matrix_list.append(cu5zr5_neighbor_node_number_matrix_list[i])\n",
    "\n",
    "for i in range(len(ni5al5_neighbor_node_number_matrix_list)):\n",
    "\n",
    "    neighbor_node_number_matrix_list.append(ni5al5_neighbor_node_number_matrix_list[i])\n",
    "\n",
    "\n",
    "atom_number = 4000\n",
    "\n",
    "gdf = GaussianDistance(dmin=0, dmax=8, step=0.2)\n",
    "\n",
    "edge_feature_matrix_gdf_list = []\n",
    "\n",
    "#print(edge_feature_matrix_list[0][1].size)\n",
    "\n",
    "#print(edge_feature_matrix_list)\n",
    "\n",
    "#print('edge_feature_matrix_list[0][0].size',edge_feature_matrix_list[0][0].size)\n",
    "\n",
    "for sample in range(len(edge_feature_matrix_list)):#200*4000*25*1\n",
    "\n",
    "    edge_feature_matrix_gdf = np.zeros((atom_number, edge_feature_matrix_list[0][0].size, 41))#4000*25*41\n",
    "\n",
    "    for number in range(atom_number):\n",
    "\n",
    "        #print(edge_feature_matrix_list[sample][number].shape)\n",
    "\n",
    "        gdf_edge_feature = gdf.expand(edge_feature_matrix_list[sample][number])#25*41\n",
    "\n",
    "        #print(gdf_edge_feature.shape)\n",
    "\n",
    "        edge_feature_matrix_gdf[number] = gdf_edge_feature.reshape(edge_feature_matrix_list[0][0].size,41)#25*41\n",
    "\n",
    "    edge_feature_matrix_gdf_list.append(edge_feature_matrix_gdf)#200*4000*25*41\n",
    "\n",
    "#print(edge_feature_matrix_gdf_list[0][0])\n",
    "\n",
    "ln_Label = np.zeros((200,1))\n",
    "\n",
    "for sample in range(200):\n",
    "\n",
    "    if sample < 100:#Cu_Zr:(0,1)\n",
    "\n",
    "        ln_Label[sample][0] = 1\n",
    "\n",
    "    elif sample >= 100:#Al_Ni:(1,0)\n",
    "\n",
    "        ln_Label[sample][0] = 0\n",
    "\n",
    "\n",
    "edge_feature_matrix = np.zeros((len(edge_feature_matrix_list),atom_number,edge_feature_matrix_list[0][0].size,41))#200*4000*25*41\n",
    "\n",
    "neighbor_mask_matrix = np.zeros((len(edge_feature_matrix_list),atom_number,edge_feature_matrix_list[0][0].size))#200*4000*25\n",
    "\n",
    "neighbor_index_matrix = np.zeros((len(edge_feature_matrix_list),atom_number,edge_feature_matrix_list[0][0].size))#200*4000*25\n",
    "\n",
    "node_feature_matrix = np.zeros((len(edge_feature_matrix_list),atom_number,2))#200*4000*2\n",
    "\n",
    "for i in range(len(edge_feature_matrix_list)):#200\n",
    "\n",
    "    edge_feature_matrix[i] = edge_feature_matrix_gdf_list[i]#200*4000*25*41\n",
    "\n",
    "    neighbor_mask_matrix[i] = neighbor_node_number_matrix_list[i]#200*4000*25\n",
    "\n",
    "    neighbor_index_matrix[i] = neighbor_node_index_matrix_list[i]#200*4000*25\n",
    "\n",
    "    node_feature_matrix[i] = node_feature_matrix_list[i]#200*4000*2\n",
    "\n",
    "\n",
    "#sample_edge_feature_matrix = padding_zeros_matrix(matrix_list = edge_feature_matrix_gdf_list)\n",
    "\n",
    "#print(sample_edge_feature_matrix[0][0][0])\n",
    "\n",
    "#sample_node_feature_matrix = padding_zeros_matrix(matrix_list = node_feature_matrix_list)\n",
    "\n",
    "#sample_neighbor_node_number_matrix = padding_zeros_matrix(matrix_list = neighbor_node_number_matrix_list)\n",
    "\n",
    "#sample_neighbor_node_index_matrix = padding_zeros_matrix(matrix_list = neighbor_node_index_matrix_list)\n",
    "\n",
    "#sample_contre_node_index_matrix = padding_zeros_matrix(matrix_list = contre_node_index_matrix_list)\n",
    "\n",
    "#num_edge_number_matrix = edge_number_matrix(matrix_list = edge_feature_matrix_list)\n",
    "\n",
    "#print('num_edge_number_matrix',num_edge_number_matrix)\n",
    "\n",
    "#Cu50_GFA_label = GFA_label(sample_number=set_num_sample,thickness=1.13)\n",
    "\n",
    "sample_edge_feature_matrix_tensor = torch.from_numpy(edge_feature_matrix).float()\n",
    "\n",
    "#print(sample_edge_feature_matrix_tensor[0][0])\n",
    "\n",
    "sample_node_feature_matrix_tensor = torch.from_numpy(node_feature_matrix).float()\n",
    "\n",
    "sample_neighbor_mask_matrix_tensor = torch.from_numpy(neighbor_mask_matrix).float()\n",
    "\n",
    "sample_neighbor_node_index_matrix_tensor = torch.from_numpy(neighbor_index_matrix).float()\n",
    "\n",
    "#sample_contre_node_index_matrix_tensor = torch.from_numpy(sample_contre_node_index_matrix).float()\n",
    "\n",
    "sample_label_tensor = torch.from_numpy(ln_Label).float()\n",
    "\n",
    "batch_tensor = torch.from_numpy(batch).float()\n",
    "\n",
    "#print('batch_tensor',batch_tensor)\n",
    "\n",
    "#edge_number_matrix_trnsor = torch.from_numpy(con_edge_number_matrix).float()\n",
    "\n",
    "#GFA_label_tensor = torch.from_numpy(con_GFA_label).float()\n",
    "\n",
    "\n",
    "m_node=deepcopy(sample_node_feature_matrix_tensor[0])\n",
    "\n",
    "m_edge=deepcopy(sample_edge_feature_matrix_tensor[0])\n",
    "\n",
    "m_mask=deepcopy(sample_neighbor_mask_matrix_tensor[0])\n",
    "\n",
    "m_index=deepcopy(sample_neighbor_node_index_matrix_tensor[0])\n",
    "\n",
    "n_node=deepcopy(sample_node_feature_matrix_tensor[100])\n",
    "\n",
    "n_edge=deepcopy(sample_edge_feature_matrix_tensor[100])\n",
    "\n",
    "n_mask=deepcopy(sample_neighbor_mask_matrix_tensor[100])\n",
    "\n",
    "n_index=deepcopy(sample_neighbor_node_index_matrix_tensor[100])\n",
    "\n",
    "\n",
    "\n",
    "#尝试分出测试集\n",
    "train_lengths = 150\n",
    "\n",
    "validation_lengths = 50\n",
    "\n",
    "#交叉验证\n",
    "\n",
    "train_edge_feature_matrix_tensor, validation_edge_feature_matrix_tensor = random_split(dataset=sample_edge_feature_matrix_tensor,\n",
    "                                                                   lengths=[train_lengths,validation_lengths],\n",
    "                                                                   generator=torch.Generator().manual_seed(5))\n",
    "\n",
    "train_node_feature_matrix_tensor, validation_node_feature_matrix_tensor = random_split(dataset=sample_node_feature_matrix_tensor,\n",
    "                                                                   lengths=[train_lengths,validation_lengths],\n",
    "                                                                   generator=torch.Generator().manual_seed(5))\n",
    "\n",
    "train_neighbor_node_mask_matrix_tensor, validation_neighbor_node_mask_matrix_tensor = random_split(dataset=sample_neighbor_mask_matrix_tensor,\n",
    "                                                                   lengths=[train_lengths,validation_lengths],\n",
    "                                                                   generator=torch.Generator().manual_seed(5))\n",
    "\n",
    "train_neighbor_node_index_matrix_tensor, validation_neighbor_node_index_matrix_tensor = random_split(dataset=sample_neighbor_node_index_matrix_tensor,\n",
    "                                                                    lengths=[train_lengths,validation_lengths],\n",
    "                                                                    generator=torch.Generator().manual_seed(5))\n",
    "\n",
    "#train_contre_node_index_matrix_tensor, validation_contre_node_index_matrix_tensor = random_split(dataset=sample_contre_node_index_matrix_tensor,\n",
    "#                                                                    lengths=[train_lengths,validation_lengths],\n",
    "#                                                                    generator=torch.Generator().manual_seed(7))\n",
    "\n",
    "\n",
    "train_sample_label_tensor , validation_sample_label_tensor = random_split(dataset=sample_label_tensor,\n",
    "                                                                    lengths=[train_lengths,validation_lengths],\n",
    "                                                                    generator=torch.Generator().manual_seed(5))\n",
    "\n",
    "train_batch_tensor, validation_batch_tensor = random_split(dataset=batch_tensor,\n",
    "                                                                    lengths=[train_lengths,validation_lengths],\n",
    "                                                                    generator=torch.Generator().manual_seed(5))\n",
    "\n",
    "print('########################train_batch_tensor',train_batch_tensor)\n",
    "\n",
    "#train_GFA_label_tensor, validation_GFA_label_tensor = random_split(dataset=GFA_label_tensor,\n",
    "#                                                    lengths=[train_lengths,validation_lengths],\n",
    "#                                                    generator=torch.Generator().manual_seed(7))\n",
    "\n",
    "                                                                \n",
    "\n",
    "#print(Cu50_neighbor_number_matrix_tensor)\n",
    "\n",
    "''''normalizer = Normalizer(train_edge_feature_matrix_tensor)\n",
    "\n",
    "train_edge_feature_matrix_tensor = normalizer.norm(train_edge_feature_matrix_tensor)\n",
    "validation_edge_feature_matrix_tensor = normalizer.norm(validation_edge_feature_matrix_tensor)'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Data.TensorDataset(train_edge_feature_matrix_tensor[:] ,train_node_feature_matrix_tensor[:], \n",
    "                                  train_neighbor_node_mask_matrix_tensor[:],train_neighbor_node_index_matrix_tensor[:],\n",
    "                                  train_sample_label_tensor[:])\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_dataset,batch_size=1,shuffle=True,num_workers=0)  #数据采样\n",
    "\n",
    "validation_dataset = Data.TensorDataset(validation_edge_feature_matrix_tensor[:] ,validation_node_feature_matrix_tensor[:], \n",
    "                                  validation_neighbor_node_mask_matrix_tensor[:],validation_neighbor_node_index_matrix_tensor[:],\n",
    "                                  validation_sample_label_tensor[:])\n",
    "validation_loader = Data.DataLoader(dataset=validation_dataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def test(model,loader):\n",
    "\n",
    "    true_out = []\n",
    "\n",
    "    Pre_out = []\n",
    "\n",
    "    Pre_loss = []\n",
    "\n",
    "    test_losses = 0\n",
    "\n",
    "    for data in loader:#20\n",
    "\n",
    "        edge_features, node_features, neighbor_masks, neighbor_indexs, label = data\n",
    "        edge_features, node_features, neighbor_masks, neighbor_indexs, label = edge_features.to(device), node_features.to(device), neighbor_masks.to(device), neighbor_indexs.to(device), label.to(device)\n",
    "\n",
    "\n",
    "        #print('##########################data[4]',data[4])\n",
    "\n",
    "        #centre_node_index_matrix_tensor = data[4].to(device)\n",
    "\n",
    "        out,a = model(node_features[0],edge_features[0],neighbor_indexs[0],neighbor_masks[0])\n",
    "\n",
    "        Pre_out.append(out)\n",
    "        \n",
    "        #print('test_pred',pred)\n",
    "        \n",
    "        y = label[0].to(device)#1*2\n",
    "\n",
    "        true_out.append(y)\n",
    "        \n",
    "        #print('test_true',y)\n",
    "        \n",
    "        loss_function = torch.nn.NLLLoss()#BCEWithLogitsLoss\n",
    "        \n",
    "        loss = loss_function(out, y.to(torch.long))\n",
    "\n",
    "        test_losses = loss + test_losses\n",
    "\n",
    "        Pre_loss.append(loss)\n",
    "        #print(pred)\n",
    "    return Pre_out,Pre_loss,true_out, test_losses\n",
    "\n",
    "\n",
    "#GNN_layer_1 = GNNLayer(node_embedding_len=2,edge_embedding_len=1,attention_len=1)\n",
    "\n",
    "#GNN_layer_2 = GNNLayer(node_embedding_len=2,edge_embedding_len=1,attention_len=1)\n",
    "\n",
    "#print(model)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GNNModel(node_feature_len=2,edge_embedding_len=41,init_node_embedding_units=[2])\n",
    "\n",
    "\n",
    "model = model.to(device) \n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Epoch = []\n",
    "\n",
    "Train_loss = []\n",
    "\n",
    "Test_loss = []\n",
    "\n",
    "RESUME = False\n",
    "\n",
    "EPOCH = 4\n",
    "\n",
    "start_epoch = -1\n",
    "\n",
    "if RESUME:\n",
    "    path_checkpoint = \"./models/checkpoint/ckpt_best_1735.pth\"  # 断点路径\n",
    "    checkpoint = torch.load(path_checkpoint)  # 加载断点\n",
    "\n",
    "    model.load_state_dict(checkpoint['net'])  # 加载模型可学习参数\n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])  # 加载优化器参数\n",
    "    start_epoch = checkpoint['epoch']  # 设置开始的epoch\n",
    "    \n",
    "\n",
    "for epoch in range(start_epoch+1, EPOCH):\n",
    "\n",
    "    print('epoch',epoch)\n",
    "\n",
    "    losses = 0\n",
    "\n",
    "    for step, data in enumerate(train_loader):\n",
    "\n",
    "        #print('step',step)\n",
    "\n",
    "\n",
    "        #data = torch.stack(data, dim=1)\n",
    "        edge_features, node_features, neighbor_masks, neighbor_indexs, label = data\n",
    "        edge_features, node_features, neighbor_masks, neighbor_indexs, label = edge_features.to(device), node_features.to(device), neighbor_masks.to(device), neighbor_indexs.to(device), label.to(device)\n",
    "\n",
    "        #4000*2\n",
    "\n",
    "        #4000*25*41\n",
    "\n",
    "        #4000*25\n",
    "\n",
    "        #4000*25\n",
    "\n",
    "        #4000*1\n",
    "\n",
    "        #print('##########################data[4]',data[4])\n",
    "\n",
    "        #centre_node_index_matrix_tensor = data[4].to(device)\n",
    "\n",
    "        out,a = model(node_features[0],edge_features[0],neighbor_indexs[0],neighbor_masks[0])\n",
    "\n",
    "        #print('out',out)\n",
    "\n",
    "        #print('y',y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_function = torch.nn.NLLLoss()\n",
    "\n",
    "\n",
    "        loss = loss_function(out, label[0].to(torch.long))\n",
    "\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        losses = losses + loss\n",
    "\n",
    "        #print('loss',loss)\n",
    "\n",
    "    #val_loss = test(model,validation_loader)\n",
    "\n",
    "    #print('val_loss', val_loss)\n",
    "\n",
    "    print('losses',losses.item())\n",
    "\n",
    "    #pccs = np.corrcoef(out.cpu().detach().numpy().reshape(1,200), y.cpu().cpu().detach().numpy().reshape(1,200))\n",
    "\n",
    "    #print('pccs',pccs)\n",
    "\n",
    "    Epoch.append(epoch)\n",
    "\n",
    "    Train_loss.append(losses.item())\n",
    "\n",
    "    out,loss,true_out,test_losses = test(model,validation_loader)\n",
    "    \n",
    "    target_tensor = torch.tensor(true_out).unsqueeze(1) \n",
    "    \n",
    "    prediction_tensor = torch.stack([torch.squeeze(out[i]) for i in range(len(out))])\n",
    "    \n",
    "    accuracy, precision, recall, fscore, auc_score = classification_eval(target_tensor, prediction_tensor)\n",
    "\n",
    "   #print(f\"Accuracy: {accuracy}\")\n",
    "    #print(f\"Precision: {precision}\")\n",
    "    #print(f\"Recall: {recall}\")\n",
    "    #print(f\"F1 Score: {fscore}\")\n",
    "    #print(f\"AUC Score: {auc_score}\")\n",
    "\n",
    "#print(out,loss,true_out)\n",
    "    print('test_losses',test_losses.item())\n",
    "    #print(out[2].cpu().detach().numpy())\n",
    "    #print(loss[2].cpu().detach().numpy())\n",
    "    #print(true_out[2].cpu().detach().numpy())\n",
    "\n",
    "    #test_pccs = np.corrcoef(out[0].cpu().detach().numpy().reshape(1,200), true_out[0].cpu().detach().numpy()[0].reshape(1,200))\n",
    "\n",
    "    #print('test_pcc',test_pccs)\n",
    "\n",
    "    Test_loss.append(test_losses.item())\n",
    "\n",
    "    if epoch %10 == 0:\n",
    "\n",
    "\n",
    "        #print('epoch', epoch)\n",
    "\n",
    "        #print('loss',loss)\n",
    "        #print('out',out)\n",
    "        #print('y',y)\n",
    "\n",
    "        print('save_epoch:',epoch)\n",
    "\n",
    "        checkpont = {\n",
    "            \"net\": model.state_dict(),\n",
    "            'optimizer':optimizer.state_dict(),\n",
    "            \"epoch\":epoch\n",
    "        }\n",
    "        #if not os.path.isdir(\"./models/checkpoint\"):\n",
    "            #os.mkdir('./models/checkpoint')\n",
    "        #torch.save(checkpont,'./models/checkpoint/ckpt_best_%s.pth'%(str(epoch)))\n",
    "\n",
    "\n",
    "\n",
    "    if epoch == 3:\n",
    "\n",
    "        torch.save(model.state_dict(),'test.pth')\n",
    "\n",
    "#import seaborn as sns  # 导入Seaborn库\n",
    "\n",
    "\n",
    "alpha_value = 0.5 \n",
    "# 随机生成[0, 4000]范围内的100个不重复的整数\n",
    "random_numbers = random.sample(range(0, 4001), 100)\n",
    "\n",
    "print(random_numbers)\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "# 初始化图\n",
    "G = nx.Graph()\n",
    "\n",
    "# 假设model, n_node, n_edge, n_index, n_mask已经定义并准备好\n",
    "j2, attention_weight = model(m_node.to(device), m_edge.to(device), m_index.to(device), m_mask.to(device))\n",
    "\n",
    "# 设置一个权重的阈值，这里我们选择仅显示最重要的10%的边\n",
    "all_weights = attention_weight.view(-1).cpu().detach().numpy()  # 转换权重为1D numpy数组\n",
    "threshold = np.percentile(all_weights, 95)  # 计算90%分位数作为阈值\n",
    "\n",
    "# 初始化图，这次我们只添加权重大于等于阈值的边\n",
    "for i in range(4000):  # 假设N和n_index.shape[1]已经定义\n",
    "    for j in range(m_index.shape[1]):\n",
    "        node1 = i\n",
    "        node2 = m_index[i, j].item()\n",
    "        weight = attention_weight[i, j, 0].item()\n",
    "        \n",
    "        if weight >= threshold:  # 仅当权重大于等于阈值时，添加边\n",
    "            G.add_node(node1)  # 添加节点，networkx会自动忽略重复的节点\n",
    "            G.add_node(node2)\n",
    "            G.add_edge(node1, node2, weight=weight)  # 添加带权重的边\n",
    "\n",
    "# 创建一个更大的图形来绘制\n",
    "plt.figure(figsize=(100, 100))\n",
    "\n",
    "# 计算节点位置\n",
    "pos= nx.spring_layout(G, k=0.3, scale=2.0, iterations=10)\n",
    "\n",
    "# 绘制节点\n",
    "nx.draw_networkx_nodes(G, pos, node_color='red', node_size=100)\n",
    "\n",
    "# 绘制过滤后的边，调整边的粗细基于权重\n",
    "edges = G.edges(data=True)\n",
    "for (node1, node2, data) in edges:\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=[(node1, node2)], width=data['weight']*40)\n",
    "\n",
    "# 移除坐标轴\n",
    "plt.axis('off')\n",
    "\n",
    "# 保存图形\n",
    "plt.savefig('11.png')\n",
    "\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "\n",
    "# 初始化图\n",
    "G = nx.Graph()\n",
    "\n",
    "# 假设model, n_node, n_edge, n_index, n_mask已经定义并准备好\n",
    "j2, attention_weight = model(n_node.to(device), n_edge.to(device), n_index.to(device), n_mask.to(device))\n",
    "\n",
    "# 设置一个权重的阈值，这里我们选择仅显示最重要的10%的边\n",
    "all_weights = attention_weight.view(-1).cpu().detach().numpy()  # 转换权重为1D numpy数组\n",
    "threshold = np.percentile(all_weights, 95)  # 计算90%分位数作为阈值\n",
    "\n",
    "# 初始化图，这次我们只添加权重大于等于阈值的边\n",
    "for i in range(4000):  # 假设N和n_index.shape[1]已经定义\n",
    "    for j in range(n_index.shape[1]):\n",
    "        node1 = i\n",
    "        node2 = n_index[i, j].item()\n",
    "        weight = attention_weight[i, j, 0].item()\n",
    "        \n",
    "        if weight >= threshold:  # 仅当权重大于等于阈值时，添加边\n",
    "            G.add_node(node1)  # 添加节点，networkx会自动忽略重复的节点\n",
    "            G.add_node(node2)\n",
    "            G.add_edge(node1, node2, weight=weight)  # 添加带权重的边\n",
    "\n",
    "# 创建一个更大的图形来绘制\n",
    "plt.figure(figsize=(100, 100))\n",
    "\n",
    "# 计算节点位置\n",
    "pos= nx.spring_layout(G, k=0.3, scale=2.0, iterations=10)\n",
    "\n",
    "# 绘制节点\n",
    "nx.draw_networkx_nodes(G, pos, node_color='yellow', node_size=100)\n",
    "\n",
    "# 绘制过滤后的边，调整边的粗细基于权重\n",
    "edges = G.edges(data=True)\n",
    "for (node1, node2, data) in edges:\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=[(node1, node2)], width=data['weight']*40)\n",
    "\n",
    "# 移除坐标轴\n",
    "plt.axis('off')\n",
    "\n",
    "# 保存图形\n",
    "plt.savefig('12.png')\n",
    "\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "# 假设Train_loss是需要梯度的张量\n",
    "plt.plot(Epoch, Train_loss, c='r', label='Train Loss')\n",
    "# 假设Train_loss是需要梯度的张量\n",
    "plt.plot(Epoch, Test_loss, c='b', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train vs Test Loss')\n",
    "plt.legend()\n",
    "plt.savefig('test.png')\n",
    "print(\"##################################\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78f458-956c-4a0b-872f-17a161da09fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
